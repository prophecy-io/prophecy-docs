---
title: 'Test definitions'
description: 'Create reusable data tests that can be applied to different models'
---

Test definitions are generic data tests that ... Each project can contain multiple test definitions.

<Note>
  Test definitions become [dbt generic data
  tests](https://docs.getdbt.com/docs/build/data-tests#generic-data-tests) behind the scenes. When
  you add a test definition to a table, Prophecy add the test definition as a property of the dbt
  model.
</Note>

<Info>SQL fabrics configured with BigQuery and a CMEK are not compatible with data tests.</Info>

## Default test definitions

Prophecy includes a set of default test definitions that you can use to get started.

- **Unique**: Validates that each value within a column is unique.
- **Not null**: Validates that a column contains no null values.
- **Accepted values**: Validates that column values only include values from a defined set.
- **Relationships**: Validates referential integrity by ensuring that each value in a column exists as a corresponding value in another column or model.

## How are test definitions defined?

Each test definition is a SQL query that is executed to validate the data.

Each test has a `model` parameter by default. This model is automatically set to the model that you apply the test to.

For example, the `unique` test definition query selects duplicate values; if no duplicates are found, the test passes. Another example is the `relationships` test definition query selects rows where the foreign key value has no matching primary key; if all values have matches, the test passes.

You'll have to be familiar with SQL to create your own test definitions.

dbt tests fail when they return rows, and pass when they return no rows.

### Example

Name: `equal_rowcount`

Parameters:

| Name            | Type    |
| --------------- | ------- |
| `model`         | `table` |
| `compare_model` | `table` |

Definition:

```sql
with a as (
    select
      count(*) as count_a
    from {{ model }}
),
b as (
    select
      count(*) as count_b
    from {{ compare_model }}
),
final as (
    select
        count_a,
        count_b,
        abs(count_a - count_b) as diff_count
    from a
    cross join b
)
select * from final
where diff_count > 0
```

## Build a test

Use the following example to learn how to build a test definition.

### Add a test definition

To add a test definition, follow these steps:

1. In the left sidebar, click **+ Add Entity**.
1. Hover the **Tests** option and select **Test definitions**.
1. Assign a name to your test definition, such as `not_constant`.
1. Keep the default path `tests/generic`. This is the directory in the Git repository where Prophecy will store the test definition.
1. Click **Create**. The test definition page opens.

<Tip>
  You can also create a new data test directly from the **Data Tests** tab of a table or model gem.
</Tip>

### Define the test query

On the test definition page, you can define the test query.

1. In the **Description**, add a summary of what the test definition does, for example `Validates that a column does not have the same value in all rows.`
1. In the **Parameters** table, add the parameters for the test. By default, the `model` parameter of type `table`is added. The `not_constant` test also requires a reference to a column in the model. Thus, you need to add another parameter called `column_name` of type `column`.
1. In the **Definition** field, add the SQL query for the test. In this case:

   ```sql
   select
      count(distinct {{ column_name }}) as filler_column
   from {{ model }}
   having count(distinct {{ column_name }}) = 1
   ```

![Create a new model test definition](/data-analytics/development/tests/img/test-not-constant.png)

<Warning>

If changes are made to the columns or schemas used in your data test, then Prophecy will delete the data test. For example, if you run into a data mismatch error on the Schema tab of your target model or update the schema, then your data test will be affected.

</Warning>

## Run tests

After you've developed your model or column test, you can run it.

### Assign the test definition to a table or model

1. Open the table or model that you want to run the test on.
1. Click the **Data Tests** tab.
1. Click **+ New Test**.
1. Under **Data Test Type**, select the test definition you want to add to the gem.
1. Fill in the parameters for the test.
1. Click **Create Test**.

If necessary, you can add additional tests to the same table or model.

### Run data tests

Once you have assigned test definitions to a table or model, you can run them.

1. Select the test definition you want to run.
1. Click **Run all** to execute all the defined tests.
1. Once the tests finish running, you'll see the relevant test result next to each test definition.

![Assign a test definition to a table or model](/data-analytics/development/tests/img/test-table-config.png)

<Info>
  Click **View Log** to view the test logs. You'll only be able to view the log of the most recent
  test run. You can copy or download the logs if needed.
</Info>

<Accordion title="Schedule test runs (Models ONLY)">

When you schedule a project, you can also schedule tests in the project to run. Scheduling tests ensures that your data is correct on a regular basis.

1. In the left sidebar of the project, click **+ Add Entity**.
1. Click **Job**. This opens the **Create Job** dialog.
1. Enter a name for your job and click **Create New**.
1. Drag a **Model** gem to your visual canvas.
1. Click the model to open the model properties.
1. Select the database object you want to run the test on.
1. Select the **Run tests** checkbox in the left sidebar of the model gem.
1. Ensure that your **project, model**, and **fabric** are correct.
1. Click **Save**.

</Accordion>

## Advanced settings

In additional to the required test arguments, you can also set the following advanced settings for all tests:

| Setting                    | Description                                                                                                                                                                                                                                                                                 |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Filter Condition**       | When enabled, lets you define a filter expression for what you want your test to run on. Use the dropdown to select an expression.                                                                                                                                                          |
| **Severity**               | Determines whether the failure of the test returns an error or warning. Select from the dropdown to set the severity level.                                                                                                                                                                 |
| **Failure Calculation**    | Sets the failure condition used to run against the test result. Use the expression builder to define how failures are calculated, such as using the `count()` function on a column or multiple columns.                                                                                     |
| **Error If**               | Sets conditions that trigger an error. Use the expression builder to define when the test should return an error based on the failure calculation result.                                                                                                                                   |
| **Warn If**                | Sets conditions that trigger a warning. Use the expression builder to define when the test should return a warning based on the failure calculation result.                                                                                                                                 |
| **Store Failures**         | When enabled, stores all records that failed the test. The records are saved in a new table with schema `dbt_test__audit` in your database. The table is named after the name of the model and data test. Make sure you have write permission to create a new table in your data warehouse. |
| **Set max no of failures** | When enabled, sets the maximum number of failures returned by a test query. You can set the limit to save resources and time by having the test stop its query as soon as it encounters a certain number of failed rows.                                                                    |

<Info>
  Severity operates from highest priority (error) to lowest priority (warning). If you select
  **error**, the test first checks the **Error If** conditions. If no errors are found, it then
  checks the **Warn If** conditions. If you select **warning**, the test only checks the **Warn If**
  conditions. If you don't select a severity, **error** is chosen by default.
</Info>

## Sharing test definitions

If you publish your project as a package, you can share your test definitions with other teams. Once they import the package, they will be able to use your test definitions in their own projects.
