---
title: Databricks SQL
description: Run models on a Databricks SQL warehouse
---

<Callout icon="/images/icon.png" color="#FFC107">
  Available only in [Enterprise Edition](/data-engineering/administration/platform/editions).
</Callout>

To run models on Databricks, you need to create a fabric with a Databricks connection.

<Warning>
  SQL fabrics are for **models only**. Create a [Prophecy
  fabric](/data-analytics/environment/fabrics/prophecy-fabrics) with a Databricks SQL warehouse
  connection to run pipelines.
</Warning>

## Create a fabric

Fabrics define your Prophecy project execution environment. To create a new fabric:

1. Click on the **Create Entity** button from the left navigation bar.
1. Click on the **Fabric** tile.

## Basic Info

Next, complete the fields in the **Basic Info** page.

1. Provide a fabric title. It can be helpful to include descriptors like `dev` or `prod` in your title.
1. (Optional) Provide a fabric description.
1. Select a [team](/data-analytics/administration/management/teams/teams) to own this fabric. Open the dropdown to see the teams you belong to.
1. Click **Continue**.

![DBInfo](/data-engineering/fabrics/sql-provider/img/DatabricksFabric1.png)

## Provider

The SQL provider is both the storage warehouse and the execution environment where your SQL code will run. To configure the provider:

1. Select **SQL** as the Provider type.
1. Click the dropdown menu for the list of supported Provider types, and select **Databricks**.
1. Copy the **JDBC URL** from the Databricks UI as shown. This is the URL that Prophecy will connect for SQL Warehouse data storage and execution. <br/><br/>
   <Note>If using self-signed certificates, add `AllowSelfSignedCerts=1` to your JDBC URL.</Note>
1. Select [Personal Access Token](https://docs.databricks.com/aws/en/dev-tools/auth/pat) or OAuth (recommended) for authentication with Databricks.
1. Enter the catalog and schema to be used as the default write location for target models.
1. Click **Continue**.

![SFProvider](/data-engineering/fabrics/sql-provider/img/DatabricksFabric2.png)

Prophecy respects **individual user credentials** when accessing Databricks catalogs, tables, databases, etc.

<Note>
  Prophecy supports Databricks Volumes. When you run a Python or Scala pipeline via a job, you must
  bundle them as whl/jar artifacts. These artifacts must then be made accessible to the Databricks
  job in order to use them as a library installed on the cluster. You can designate a path to a
  Volume for uploading the whl/jar files under Artifacts.
</Note>

### Optional: Connections

If you want to crawl your warehouse metadata on a regular basis, you can set a connection here.
